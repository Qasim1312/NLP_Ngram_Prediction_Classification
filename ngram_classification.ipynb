{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize  # Ensure nltk is installed and word_tokenize is downloaded\n",
    "import nltk\n",
    "import string \n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# removing all the stopwords form the eng dictionary to make this more efficient and run normally (my laptop can absolutely not handle this big of a dataset)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# we're gonna read the corpus from the file where i placewd it\n",
    "file_path = r'' \n",
    "\n",
    "preparing_processed_data = \"\"  #here we will store the initial processed data\n",
    "\n",
    "# now we will read and preprocess line by line\n",
    "with open(file_path, 'r', encoding='utf-8') as file: #the syntax of file handling\n",
    "    for line in file:\n",
    "        # we're going to convert to lowercase here, makes it easier\n",
    "        lowercase = line.lower()\n",
    "        \n",
    "        # tokenizing it according to the syntax\n",
    "        tokens = word_tokenize(lowercase)\n",
    "        \n",
    "        # here we will clean the punctuation\n",
    "        tokens_cleaned = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "        \n",
    "        # collecting it into a single string here\n",
    "        cleaned_text = ' '.join(tokens_cleaned)\n",
    "        \n",
    "        # initialized this to save the data here so why not, could definitely use cleaned text in itself as well\n",
    "        preparing_processed_data += cleaned_text + ' '\n",
    "\n",
    "# now we save it all to the file here, albeit not necessary\n",
    "output_file_path = r''\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(preparing_processed_data)\n",
    "\n",
    "print(f\"Preprocessed data has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    words = content.split()  # testing splitting here\n",
    "    print(words[:5])  # printing 5 words to see if it works (debugging)\n",
    "\n",
    "\n",
    "output_file=output_file_path\n",
    "processed_data=preparing_processed_data #I dont't wanna even accidentally alter the original processed datasets because it takes about an hr just to compile all this, cannot risk any further risks\n",
    "\n",
    "with open(output_file, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    words = content.split()  # testing it with the new data because why not\n",
    "    print(words[:5])  # printing 5 as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# tokenizing the processed text into 'preparing_processed_data'\n",
    "tokens = word_tokenize(processed_data)\n",
    "\n",
    "# testing 10 this time just to see how it works and if everything is fine\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that that's done, we create a unique array, basically i plan on creating an array that stores every word but it should be unique, and another array that keeps the count of the unique array, similar indexing to keep track of which word was repeated how many times, ill explain this in the demo if need be\n",
    "\n",
    "unique_data = []\n",
    "count_of_unique = []\n",
    "\n",
    "\n",
    "for word in tokens: #loop going through every word in processed_data\n",
    "    if word in unique_data:\n",
    "        index = unique_data.index(word) #if the word has once come before and exists in the unique_data array, increase it's count  by one\n",
    "        count_of_unique[index] += 1\n",
    "    else:\n",
    "        unique_data.append(word) #if it's a new word, append it into the unique_data and make it's count 1 (exists once)\n",
    "        count_of_unique.append(1)\n",
    "\n",
    "#printing both to se ehow they work\n",
    "print(\"the unique data is :\", unique_data)\n",
    "print(\"count of corresponding unique data is:\", count_of_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ount_of_unique_2=count_of_unique\n",
    "unique_data_2=unique_data #once again, saving because i cannot afford to lose this even accidentally\n",
    "\n",
    "\n",
    "#so basically, now we consider the order, previously we just indexed according to the appends and didnt touch that so we dont have any issues with the append, now we can put it in order to see which one has the highest repetition and all, once again i can explain in the demo\n",
    "\n",
    "#initializing arrays\n",
    "unique_cells_in_order = []\n",
    "count_of_ordered_cells = []\n",
    "\n",
    "#creating copies because i dont trust myself\n",
    "unique_data_copy = unique_data[:]\n",
    "count_of_unique_copy = count_of_unique[:]\n",
    "\n",
    "\n",
    "while len(count_of_unique_copy) > 0: #loop until count_of_unique_copy is completed\n",
    "    #find the current highest value and append it here\n",
    "    max_index = count_of_unique_copy.index(max(count_of_unique_copy))\n",
    "    \n",
    "    #we use the same index and find the same word in the unique data array and append that as well\n",
    "    unique_cells_in_order.append(unique_data_copy[max_index])\n",
    "    count_of_ordered_cells.append(count_of_unique_copy[max_index])\n",
    "    \n",
    "    #remove that word so we can work on the next maximum valued word\n",
    "    del unique_data_copy[max_index]\n",
    "    del count_of_unique_copy[max_index]\n",
    "\n",
    "#testing once again to see if it worked\n",
    "print(\"herre are the unique cells, but in order:\", unique_cells_in_order)\n",
    "print(\"the count of those ordered unique cells are as follows :\", count_of_ordered_cells)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram is completed till above this cell, gonna waste some cells now coz i want a clear difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the processed texts into words once again\n",
    "words = preparing_processed_data.split()  \n",
    "\n",
    "# now we'll create a dictionary to store bigrams and their counts, needed becuse double words will be counted at once here andthis is convenient\n",
    "bigram_counts = {}\n",
    "\n",
    "\n",
    "for i in range(len(words) - 1):  # consider the list of words as the limit as we need this much for the loop, and -1 because we dont want any indexing errors since we're using 2 words at once anyways, would be -2 for trigrams and so on\n",
    "    bigram = (words[i], words[i+1])  #2 words in a row\n",
    "    \n",
    "    #now we count how many times it exists, if it doesnt then we make it as 1 and index the new word\n",
    "    if bigram in bigram_counts:\n",
    "        bigram_counts[bigram] += 1\n",
    "    else:\n",
    "        bigram_counts[bigram] = 1\n",
    "\n",
    "# checking it out right now\n",
    "print(\"Bigram Counts:\", bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = preparing_processed_data.split()  \n",
    "\n",
    "# now we'll create a dictionary to store bigrams and their counts, needed becuse double words will be counted at once here andthis is convenient\n",
    "bigram_counts = {}\n",
    "\n",
    "\n",
    "for i in range(len(words) - 1):  # consider the list of words as the limit as we need this much for the loop, and -1 because we dont want any indexing errors since we're using 2 words at once anyways, would be -2 for trigrams and so on\n",
    "    bigram = (words[i], words[i+1])  #2 words in a row\n",
    "    \n",
    "    #now we count how many times it exists, if it doesnt then we make it as 1 and index the new word\n",
    "    if bigram in bigram_counts:\n",
    "        bigram_counts[bigram] += 1\n",
    "    else:\n",
    "        bigram_counts[bigram] = 1\n",
    "\n",
    "# checking it out right now\n",
    "bigram_sample = list(bigram_counts.items())[:110000]    # hit and trial to see what number my laptop could handle, went from 10 mil to a mil to 1 lakh to 5 lakh to 3 lakh to 2 lakh to 1.5 lakh to 1.25 lakh to 1.1 lakh to 1.15 lakh, realizing this is it\n",
    "print(\"the words and their repetitins here for bigram :\", bigram_sample)\n",
    "# now we're going to sort it in descenging order, according to the syntax we searched\n",
    "sorted_bigrams = sorted(bigram_counts.items(), key=lambda x: x[1], reverse=True)  #sorting occurs here\n",
    "\n",
    "# creating new arrays as always\n",
    "bigram_counts_unique = []   # storing the number of occurances in descending order\n",
    "bigram_unique_cell = []     # storing the bigrams themsleves\n",
    "\n",
    "# now we just append in here\n",
    "for bigram, count in sorted_bigrams:\n",
    "    bigram_unique_cell.append(bigram)  \n",
    "    bigram_counts_unique.append(count)  \n",
    "\n",
    "# now we'll print the first 10 to see if it's working or not\n",
    "print(\"the top 10 bigrams in descending order are as follows:\")\n",
    "for i in range(10):\n",
    "    print(f\"Bigram: {bigram_unique_cell[i]}, Count: {bigram_counts_unique[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we make one for trigram now\n",
    "trigram_counts = {}\n",
    "\n",
    "#same as bigram + one more step\n",
    "for i in range(len(words) - 2):  # -2 as explained in bigram, because we cant afford errors due to indexing\n",
    "    trigram = (words[i], words[i + 1], words[i + 2])  # 3 words ina  rowwwww\n",
    "    \n",
    "    # checking if we need to increment the occurances or this is good enough\n",
    "    if trigram in trigram_counts:\n",
    "        trigram_counts[trigram] += 1\n",
    "    else:\n",
    "        trigram_counts[trigram] = 1\n",
    "\n",
    "# printing the first 10ones, ezpz\n",
    "trigram_sample = list(trigram_counts.items())[:10]  \n",
    "print(\"Sample of Trigram Counts:\", trigram_sample)\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "file_path = r''\n",
    "\n",
    "reviews = []\n",
    "sentiment = []\n",
    "\n",
    "# so we have columns named reviews and labels\n",
    "with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # we skip any headaers if there may be anuy\n",
    "    for row in reader:\n",
    "        reviews.append(row[0])  #first column is revuews\n",
    "        sentiment.append(row[1])   # second columb is sentiment\n",
    "\n",
    "print(f\"these are the reviews : {reviews[:3]}\")\n",
    "print(f\"the sentiments are as follows : {sentiment[:3]}\")\n",
    "\n",
    "\n",
    "#now i purposelly change the training model to help train the model, by changing the probability\n",
    "split_ratio = 0.8  # 80% training, 20% testing\n",
    "split_index = int(len(reviews) * split_ratio)\n",
    "\n",
    "train_reviews = reviews[:split_index]\n",
    "test_reviews = reviews[split_index:]\n",
    "\n",
    "train_sentiment = sentiment[:split_index]\n",
    "test_sentiment = sentiment[split_index:]\n",
    "\n",
    "print(f\"so the number of times we'll train reviews is: {len(train_reviews)}\")\n",
    "print(f\"no. of times we'll test the reviews is : {len(test_reviews)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# we're just initializing the frequencies here, for positiev and negatuive\n",
    "unigram_freq_positive = defaultdict(int)\n",
    "unigram_freq_negative = defaultdict(int)\n",
    "bigram_freq_positive = defaultdict(int)\n",
    "bigram_freq_negative = defaultdict(int)\n",
    "trigram_freq_positive = defaultdict(int)\n",
    "trigram_freq_negative = defaultdict(int)\n",
    "\n",
    "# training the models here\n",
    "for i, review in enumerate(train_reviews):\n",
    "    words = review.split()  # pehle we tokenize the reviews\n",
    "    \n",
    "    # now we choose the appropriate dictionary according to the sentiment\n",
    "    if train_sentiment[i] == 'positive':\n",
    "        for word in words:\n",
    "            unigram_freq_positive[word] += 1\n",
    "        \n",
    "        for j in range(len(words) - 1):\n",
    "            bigram = (words[j], words[j + 1])\n",
    "            bigram_freq_positive[bigram] += 1\n",
    "            \n",
    "        for j in range(len(words) - 2):\n",
    "            trigram = (words[j], words[j + 1], words[j + 2])\n",
    "            trigram_freq_positive[trigram] += 1\n",
    "    else:\n",
    "        for word in words:\n",
    "            unigram_freq_negative[word] += 1\n",
    "        \n",
    "        for j in range(len(words) - 1):\n",
    "            bigram = (words[j], words[j + 1])\n",
    "            bigram_freq_negative[bigram] += 1\n",
    "            \n",
    "        for j in range(len(words) - 2):\n",
    "            trigram = (words[j], words[j + 1], words[j + 2])\n",
    "            trigram_freq_negative[trigram] += 1\n",
    "\n",
    "print(\"The training for all the models has been done.\")\n",
    "\n",
    "\n",
    "\n",
    "def classify_review(review):\n",
    "    words = review.split()\n",
    "    \n",
    "    # calculatine poitive or negative\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    \n",
    "    # scores for unigrams\n",
    "    for word in words:\n",
    "        positive_score += unigram_freq_positive.get(word, 0)\n",
    "        negative_score += unigram_freq_negative.get(word, 0)\n",
    "    \n",
    "    # scores for bigrams\n",
    "    for j in range(len(words) - 1):\n",
    "        bigram = (words[j], words[j + 1])\n",
    "        positive_score += bigram_freq_positive.get(bigram, 0)\n",
    "        negative_score += bigram_freq_negative.get(bigram, 0)\n",
    "\n",
    "    # scores for trigrams\n",
    "    for j in range(len(words) - 2):\n",
    "        trigram = (words[j], words[j + 1], words[j + 2])\n",
    "        positive_score += trigram_freq_positive.get(trigram, 0)\n",
    "        negative_score += trigram_freq_negative.get(trigram, 0)\n",
    "    \n",
    "    # if positive is greater, positive, or it's negative\n",
    "    return 'positive' if positive_score > negative_score else 'negative'\n",
    "\n",
    "# tseting it on the test set to make sure ots working\n",
    "predicted_sentiment = [classify_review(review) for review in test_reviews]\n",
    "\n",
    "print(f\"the predictins are as follows: {predicted_sentiment[:5]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
